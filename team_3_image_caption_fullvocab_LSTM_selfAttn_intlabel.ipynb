{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFy5tCHU1Dre"
   },
   "outputs": [],
   "source": [
    "## 이미지 캡셔닝 1차 모델 구축\n",
    "  ## 베이스 아키텍처: Show, Attend and Tell\n",
    "  ## 추가 확보 데이터: vocab(from pre-trained model)\n",
    "  ## 프로세스\n",
    "    ## 데이터 로드, 체크\n",
    "    ## 텍스트 데이터 추가 전처리(정규화, 토큰화, 패딩)\n",
    "    ## 이미지 데이터 전처리\n",
    "    ## 모델 정의: CNN, Attention, LSTM\n",
    "    ## 기타 사용자 정의 함수 생성(손실함수 등)\n",
    "    ## 모델 컴파일 및 훈련\n",
    "    ## 모델 검증(테스트 데이터 확인, 학습결과 시각화, 매트릭스 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJrYhkpbmpC8",
    "outputId": "bdd8ef32-e1e0-4cc8-b132-d0ed6ec0a7d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\kkm_ai_study90\\anaconda3\\envs\\team_third\\lib\\site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hz4qQCm1PY1",
    "outputId": "625aa162-ffcd-4c70-d042-903e07f41330"
   },
   "outputs": [],
   "source": [
    "## Data load\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "zip_file_path = '/content/drive/My Drive/data-team3-imagecaption/train_sample_2.zip'\n",
    "%mkdir 'train_sample_2'\n",
    "\n",
    "!cp \"{zip_file_path}\" \"/content/\"\n",
    "\n",
    "!unzip -q \"/content/train_sample_2.zip\" -d \"/content/train_sample_2/\"\n",
    "\n",
    "print('unizipped files: ', os.listdir('/content/train_sample_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WELxEl8t1PbJ"
   },
   "outputs": [],
   "source": [
    "## text data additional preprocessing for model train\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "vim8MBxX1Pde",
    "outputId": "5d795f65-db2e-43c1-afd6-ac851e695dbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>file_name</th>\n",
       "      <th>category</th>\n",
       "      <th>sentence_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446250</td>\n",
       "      <td>1920</td>\n",
       "      <td>1440</td>\n",
       "      <td>IMG_0446250_person(person).jpg</td>\n",
       "      <td>person</td>\n",
       "      <td>a man is holding an umbrella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>446251</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>IMG_0446251_person(person).jpg</td>\n",
       "      <td>person</td>\n",
       "      <td>a man is turning on a gas stove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  height  width                       file_name category  \\\n",
       "0  446250    1920   1440  IMG_0446250_person(person).jpg   person   \n",
       "1  446251    1920   1080  IMG_0446251_person(person).jpg   person   \n",
       "\n",
       "                       sentence_en  \n",
       "0     a man is holding an umbrella  \n",
       "1  a man is turning on a gas stove  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./train_sample_2000.csv', encoding='utf-8')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NwUJiLDo1Pko"
   },
   "outputs": [],
   "source": [
    "## text data preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_and_tokenize(df, text_column='sentence_en'):\n",
    "  def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "  start_token = '<start>'\n",
    "  end_token = '<end>'\n",
    "  pad_token = '<pad>'\n",
    "  unk_token = '<unk>'\n",
    "\n",
    "  df['cleaned_sentence'] = df[text_column].apply(clean_text)\n",
    "\n",
    "  tokenizer = Tokenizer(oov_token=unk_token)\n",
    "  tokenizer.fit_on_texts(df['cleaned_sentence'])\n",
    "\n",
    "  tokenizer.word_index[start_token] = len(tokenizer.word_index) + 1\n",
    "  tokenizer.word_index[end_token] = len(tokenizer.word_index) + 1\n",
    "  tokenizer.word_index[pad_token] = 0\n",
    "  tokenizer.word_index[unk_token] = len(tokenizer.word_index) + 1\n",
    "  tokenizer.index_word[0] = pad_token\n",
    "  tokenizer.index_word[len(tokenizer.word_index)] = unk_token\n",
    "\n",
    "  sequences = tokenizer.texts_to_sequences(df['cleaned_sentence'])\n",
    "  sequences = [[tokenizer.word_index[start_token]] + seq + [tokenizer.word_index[end_token]] for seq in sequences]\n",
    "\n",
    "  max_length = max(len(sequence) for sequence in sequences)\n",
    "  sequences_padded = pad_sequences(sequences, maxlen=max_length, padding='post', value=tokenizer.word_index[pad_token])\n",
    "\n",
    "  vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "  x_data_text = sequences_padded[:, :-1]\n",
    "  y_data = sequences_padded[:, 1:]\n",
    "\n",
    "  y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)\n",
    "  max_sequence_length = max_length - 1\n",
    "\n",
    "  return x_data_text, y_data_one_hot, y_data, vocab_size, tokenizer, max_sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fbfu12_E1P2d"
   },
   "outputs": [],
   "source": [
    "## image data preprocessing for model train\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "image_folder = './image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-yfzTuUW1P5R"
   },
   "outputs": [],
   "source": [
    "def process_image(file_name, target_size=(299,299)):\n",
    "  image_path = os.path.join(image_folder, file_name)\n",
    "  image = load_img(image_path, target_size=target_size)\n",
    "  image = img_to_array(image)\n",
    "  image = image.astype('float32')\n",
    "  image /= 255.0\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FDaLzxHJ1P8E"
   },
   "outputs": [],
   "source": [
    "image_data = np.array([process_image(file_name) for file_name in df['file_name']])\n",
    "\n",
    "x_data_text, y_data_one_hot, y_data, vocab_size, tokenizer, max_sequence_length = preprocess_and_tokenize(df)\n",
    "x_data_image = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oiPZIrWf1P-k",
    "outputId": "af8f7785-8dbe-43c4-e28d-3ea7f88cf51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pXlZz-UJCueE"
   },
   "outputs": [],
   "source": [
    "## first model train: sample 200\n",
    "\n",
    "# x_train_image = x_data_image[:200]\n",
    "# x_train_text = x_data_text[:200]\n",
    "# y_train = y_data_one_hot[:200]\n",
    "# y_train_int = y_data[:200]\n",
    "\n",
    "x_train_image = x_data_image\n",
    "x_train_text = x_data_text\n",
    "y_train = y_data_one_hot\n",
    "y_train_int = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LfJODH4Z1PxK"
   },
   "outputs": [],
   "source": [
    "## train_test data split: one-hot encoding\n",
    "\n",
    "train_image_1, test_image_1, train_sequences_1,\\\n",
    " test_sequences_1, train_y_1, test_y_1, train_y_int, test_y_int = train_test_split(x_train_image, x_train_text, y_train, y_train_int, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxT1lxio1Pz2",
    "outputId": "71e9a1d9-6bfa-46e3-b2b6-230abc77c10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1601, 299, 299, 3)\n",
      "(1601, 39)\n",
      "(401, 299, 299, 3)\n",
      "(401, 39)\n",
      "(1601, 39, 1061)\n",
      "(401, 39, 1061)\n",
      "(1601, 39)\n",
      "(401, 39)\n"
     ]
    }
   ],
   "source": [
    "print(train_image_1.shape)\n",
    "print(train_sequences_1.shape)\n",
    "print(test_image_1.shape)\n",
    "print(test_sequences_1.shape)\n",
    "print(train_y_1.shape)\n",
    "print(test_y_1.shape)\n",
    "print(train_y_int.shape)\n",
    "print(test_y_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLhsnttiUaka",
    "outputId": "85dba5b4-56ec-4ce5-802d-e35459a194dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jq8OaKtlmwVB",
    "outputId": "fcf9c27f-9e88-4cd5-99aa-4d4a978999be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kkm_lnx22/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## BLEU, ROUGE score metrics function\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_bleu(references, candidates):\n",
    "  score = 0\n",
    "  for ref, cand in zip(references, candidates):\n",
    "    ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "    cand_tokens = nltk.word_tokenize(cand.lower())\n",
    "    score += sentence_bleu([ref_tokens], cand_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "  return score / len(candidates)\n",
    "\n",
    "def calculate_rouge(references, candidates):\n",
    "  rouge = Rouge()\n",
    "  scores = rouge.get_scores(candidates, references, avg=True)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FV-wD3KM1QBX"
   },
   "outputs": [],
   "source": [
    "## define model(use cross-attention only)\n",
    "  ## cross-attention은 그냥 attention이랑 같은거라고 보면 됨\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import AdditiveAttention, MultiHeadAttention, Lambda, Masking, RepeatVector, Reshape\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHIyMZ2FKXSg"
   },
   "outputs": [],
   "source": [
    "## 학습 준비: basic model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfgspeqbQWik",
    "outputId": "0f187238-140f-4532-e171-45893780d657"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 11:06:56.844579: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:56.864937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:56.864985: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:56.867046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:56.867121: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:56.867139: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:57.968105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:57.968313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:57.968328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-16 11:06:57.968380: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 11:06:57.968416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13515 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-01-16 11:07:02.306511: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM output shape:  (None, 39, 256)\n",
      "image feature shape:  (None, 39, 256)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 39)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 299, 299, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 39, 256)              271616    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " inception_resnet_v2 (Funct  (None, 1536)                 5433673   ['input_2[0][0]']             \n",
      " ional)                                                   6                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 39, 512)              1050624   ['embedding[0][0]']           \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  393472    ['inception_resnet_v2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 39, 512)              0         ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 256)               0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 39, 256),            787456    ['dropout[0][0]']             \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 39, 256)              0         ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " additive_attention (Additi  ((None, 39, 256),            256       ['lstm_1[0][0]',              \n",
      " veAttention)                 (None, 39, 39))                        'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 39, 256)              2103552   ['lstm_1[0][0]',              \n",
      " iHeadAttention)                                                     'lstm_1[0][0]',              \n",
      "                                                                     'lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 39, 512)              0         ['additive_attention[0][0]',  \n",
      "                                                                     'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 39, 1061)             544293    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59488005 (226.93 MB)\n",
      "Trainable params: 5151269 (19.65 MB)\n",
      "Non-trainable params: 54336736 (207.28 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## model2: (cross)attention + self_attention\n",
    "\n",
    "def create_model(vocab_size, lstm_units, max_sequence_length):\n",
    "\n",
    "  inception_resnet_model = InceptionResNetV2(include_top=False,\\\n",
    "                                            weights='imagenet',\\\n",
    "                                            pooling='avg')\n",
    "  inception_resnet_model.trainable = False\n",
    "\n",
    "  image_input = Input(shape=(299,299,3))\n",
    "  image_features = inception_resnet_model(image_input)\n",
    "\n",
    "  sequence_input = Input(shape=(max_sequence_length,))\n",
    "\n",
    "  embedding = Embedding(input_dim=vocab_size,\\\n",
    "                        output_dim=256, mask_zero=True)(sequence_input)\n",
    "\n",
    "  bidirectional_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "\n",
    "  lstm_output = bidirectional_lstm(embedding)\n",
    "\n",
    "  dropout = Dropout(0.5)\n",
    "  lstm_output = dropout(lstm_output)\n",
    "\n",
    "  second_lstm = LSTM(lstm_units,\\\n",
    "                    return_sequences=True,\\\n",
    "                    return_state=True)\n",
    "  lstm_output, _, _ = second_lstm(lstm_output)\n",
    "\n",
    "  image_features_dense = Dense(lstm_units)(image_features)\n",
    "  image_features_dense = Reshape((1, lstm_units))(image_features_dense)\n",
    "  image_features_dense = Lambda(lambda x: tf.tile(x, [1, max_sequence_length, 1]))(image_features_dense)\n",
    "\n",
    "  print(\"LSTM output shape: \", lstm_output.shape)\n",
    "  print(\"image feature shape: \", image_features_dense.shape)\n",
    "\n",
    "  attention = AdditiveAttention()\n",
    "  context_vector, attention_weights = attention([lstm_output, image_features_dense], return_attention_scores=True)\n",
    "\n",
    "  self_attention = MultiHeadAttention(num_heads=8, key_dim=lstm_units)\n",
    "  self_attention_output = self_attention(query=lstm_output, value=lstm_output, key=lstm_output)\n",
    "\n",
    "  combined_attention_output = tf.keras.layers.Concatenate(axis=-1)([context_vector, self_attention_output])\n",
    "\n",
    "  dense = Dense(vocab_size, activation='softmax')\n",
    "  output = dense(combined_attention_output)\n",
    "\n",
    "  return Model(inputs=[image_input, sequence_input], outputs=output)\n",
    "\n",
    "\n",
    "model_2 = create_model(vocab_size, 256, max_sequence_length)\n",
    "initial_weights = model_2.get_weights()\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # GPU 메모리 증가를 동적으로 할당\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4KFzS0lh10W-"
   },
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.001)\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5q1-mxYk-5G",
    "outputId": "b9ebbf06-5bc6-401c-9c41-d1a6681612ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "(1601, 39)\n",
      "(401, 39)\n"
     ]
    }
   ],
   "source": [
    "print(max_sequence_length)\n",
    "print(train_sequences_1.shape)\n",
    "print(test_sequences_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZTlng9i11BS",
    "outputId": "9f06ea51-9f6c-4144-f422-4b6d56af4027",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1705370849.763971    1392 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 4060 Ti\" frequency: 2610 num_cores: 34 environment { key: \"architecture\" value: \"8.9\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 33554432 shared_memory_size_per_multiprocessor: 102400 memory_size: 14171504640 bandwidth: 288032000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2024-01-16 11:07:32.139966: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-16 11:07:32.372168: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_38/output/_24'\n",
      "2024-01-16 11:07:32.960647: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-16 11:07:36.251134: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f51f40b5550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 11:07:36.251158: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2024-01-16 11:07:36.254480: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705370856.311642    1584 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - ETA: 0s - loss: 4.7802 - accuracy: 0.0939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1705370871.028131    1392 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 4060 Ti\" frequency: 2610 num_cores: 34 environment { key: \"architecture\" value: \"8.9\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 33554432 shared_memory_size_per_multiprocessor: 102400 memory_size: 14171504640 bandwidth: 288032000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 33s 365ms/step - loss: 4.7802 - accuracy: 0.0939 - val_loss: 4.3974 - val_accuracy: 0.1160\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 11s 223ms/step - loss: 4.1450 - accuracy: 0.1082 - val_loss: 4.0810 - val_accuracy: 0.1137\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 11s 213ms/step - loss: 3.1569 - accuracy: 0.3005 - val_loss: 2.6757 - val_accuracy: 0.4721\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 11s 214ms/step - loss: 2.0090 - accuracy: 0.5628 - val_loss: 1.9245 - val_accuracy: 0.6138\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 11s 212ms/step - loss: 1.2639 - accuracy: 0.6983 - val_loss: 1.3849 - val_accuracy: 0.7411\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.8565 - accuracy: 0.7858 - val_loss: 1.1263 - val_accuracy: 0.7884\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 11s 210ms/step - loss: 0.5447 - accuracy: 0.8585 - val_loss: 1.0240 - val_accuracy: 0.8244\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 11s 212ms/step - loss: 0.3749 - accuracy: 0.8956 - val_loss: 0.8907 - val_accuracy: 0.8532\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.2421 - accuracy: 0.9342 - val_loss: 0.8342 - val_accuracy: 0.8711\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.1853 - accuracy: 0.9461 - val_loss: 0.7652 - val_accuracy: 0.8900\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.1199 - accuracy: 0.9680 - val_loss: 0.7993 - val_accuracy: 0.8916\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 11s 212ms/step - loss: 0.0881 - accuracy: 0.9749 - val_loss: 0.8014 - val_accuracy: 0.8879\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 11s 209ms/step - loss: 0.1536 - accuracy: 0.9575 - val_loss: 0.8013 - val_accuracy: 0.8945\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 11s 210ms/step - loss: 0.0731 - accuracy: 0.9795 - val_loss: 0.7994 - val_accuracy: 0.9005\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 11s 213ms/step - loss: 0.0537 - accuracy: 0.9856 - val_loss: 0.7537 - val_accuracy: 0.9079\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 11s 210ms/step - loss: 0.0496 - accuracy: 0.9850 - val_loss: 0.7652 - val_accuracy: 0.9112\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.0344 - accuracy: 0.9901 - val_loss: 0.7393 - val_accuracy: 0.9169\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 11s 210ms/step - loss: 0.0241 - accuracy: 0.9936 - val_loss: 0.7731 - val_accuracy: 0.9188\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 11s 210ms/step - loss: 0.0228 - accuracy: 0.9943 - val_loss: 0.7656 - val_accuracy: 0.9165\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.0239 - accuracy: 0.9941 - val_loss: 0.7443 - val_accuracy: 0.9161\n"
     ]
    }
   ],
   "source": [
    "## train model_2\n",
    "\n",
    "history_1 = model_2.fit([train_image_1, train_sequences_1], train_y_int,\\\n",
    "                      validation_data=([test_image_1, test_sequences_1], test_y_int),\\\n",
    "                      epochs=20, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XlG2dSvoVtm",
    "outputId": "efd9b0f8-5893-4190-90a8-b0f5fc358118"
   },
   "outputs": [],
   "source": [
    "## BLEU, ROUGE score check\n",
    "\n",
    "predicted_sequences = model.predict([test_image_1, test_sequences_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i69DEzVkpAhY"
   },
   "outputs": [],
   "source": [
    "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "def sequences_to_text(sequences):\n",
    "  text_output = []\n",
    "  for sequence in sequences:\n",
    "    sequence_text = []\n",
    "    for word_idx in sequences:\n",
    "      max_idx = np.argmax(word_idx)\n",
    "      sequence_text.append(index_to_word.get(max_idx, ''))\n",
    "    text_output.append(' '.join(sequence_text))\n",
    "  return text_output\n",
    "\n",
    "predicted_texts = sequences_to_text(predicted_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owkFKlerrOs-"
   },
   "outputs": [],
   "source": [
    "## 원-핫 인코딩된 텍스트를 다시 문장으로 변환 function\n",
    "\n",
    "def one_hot_to_text(one_hot_sequences, index_to_word):\n",
    "  text_output = []\n",
    "  for sequence in one_hot_sequences:\n",
    "    sequence_text = []\n",
    "    for word_vec in sequence:\n",
    "      max_idx = np.argmax(word_vec)\n",
    "      word = index_to_word.get(max_idx, '')\n",
    "      if word:\n",
    "        sequence_text.append(word)\n",
    "    text_output.append(' '.join(sequence_text))\n",
    "  return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlN5XFc0pAjs",
    "outputId": "f4bc46fd-42bb-4a77-d505-652e8fbcdf33"
   },
   "outputs": [],
   "source": [
    "test_y_texts = one_hot_to_text(test_y_1, index_to_word)\n",
    "\n",
    "bleu_score = calculate_bleu(test_y_texts, predicted_texts)\n",
    "rouge_score = calculate_rouge(test_y_texts, predicted_texts)\n",
    "\n",
    "print(\"BLEU Score: \", bleu_score)\n",
    "print(\"ROUGE Score: \", rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkiinUC3G3CN",
    "outputId": "99431b9d-1565-400b-9ddd-fde18ebaafeb"
   },
   "outputs": [],
   "source": [
    "## one-hot 인코딩 라벨 대신 정수화 인코딩 라벨로 학습해보기\n",
    "\n",
    "initial_weights = model_2.get_weights()\n",
    "model_2.set_weights(initial_weights)\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_3 = model_2.fit([train_image_1, train_sequences_1], train_y_int,\\\n",
    "                      validation_data=([test_image_1, test_sequences_1], test_y_int),\\\n",
    "                      epochs=30, batch_size=32, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
